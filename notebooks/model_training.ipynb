{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Hospital Readmission Prediction\n",
    "## MSDS692 - Data Science Practicum\n",
    "### Sai Teja Lakkapally\n",
    "\n",
    "This notebook focuses on training and evaluating machine learning models for predicting 30-day hospital readmissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from etl import DataETL\n",
    "from features import FeatureEngineer\n",
    "from model import ReadmissionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print(\"Step 1: Loading and preparing data...\")\n",
    "etl = DataETL()\n",
    "data = etl.run_pipeline()\n",
    "\n",
    "print(f\"Original dataset shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"Step 2: Feature engineering...\")\n",
    "feature_engineer = FeatureEngineer()\n",
    "X, y, feature_names = feature_engineer.prepare_features(data)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance\n",
    "print(\"Step 3: Handling class imbalance...\")\n",
    "X_balanced, y_balanced = feature_engineer.handle_imbalance(X, y, method='smote')\n",
    "\n",
    "print(f\"After balancing - Features shape: {X_balanced.shape}\")\n",
    "print(f\"After balancing - Target distribution:\\n{pd.Series(y_balanced).value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "print(\"Step 4: Train-test split...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training target distribution:\\n{pd.Series(y_train).value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "model_trainer = ReadmissionModel(random_state=42)\n",
    "\n",
    "# Train baseline models\n",
    "print(\"Training Baseline Models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_trainer.train_baseline_models(X_train, y_train, cv_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline model results\n",
    "print(\"Baseline Model Performance (Cross-Validation):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "baseline_results = []\n",
    "for model_name, results in model_trainer.results.items():\n",
    "    if model_name in ['logistic_regression', 'random_forest']:\n",
    "        baseline_results.append({\n",
    "            'Model': model_name.replace('_', ' ').title(),\n",
    "            'ROC-AUC Mean': f\"{results['cv_roc_auc_mean']:.4f}\",\n",
    "            'ROC-AUC Std': f\"{results['cv_roc_auc_std']:.4f}\",\n",
    "            'PR-AUC Mean': f\"{results['cv_ap_mean']:.4f}\",\n",
    "            'PR-AUC Std': f\"{results['cv_ap_std']:.4f}\"\n",
    "        })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "print(baseline_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train advanced models\n",
    "print(\"Training Advanced Models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_trainer.train_advanced_models(X_train, y_train, cv_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all model results\n",
    "print(\"All Model Performance (Cross-Validation):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_results = []\n",
    "for model_name, results in model_trainer.results.items():\n",
    "    all_results.append({\n",
    "        'Model': model_name.replace('_', ' ').title(),\n",
    "        'ROC-AUC Mean': f\"{results['cv_roc_auc_mean']:.4f}\",\n",
    "        'ROC-AUC Std': f\"{results['cv_roc_auc_std']:.4f}\",\n",
    "        'PR-AUC Mean': f\"{results['cv_ap_mean']:.4f}\",\n",
    "        'PR-AUC Std': f\"{results['cv_ap_std']:.4f}\"\n",
    "    })\n",
    "\n",
    "all_models_df = pd.DataFrame(all_results)\n",
    "print(all_models_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "print(\"Evaluating Models on Test Set...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_results = model_trainer.evaluate_models(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display test results\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_performance = []\n",
    "for model_name, results in test_results.items():\n",
    "    test_performance.append({\n",
    "        'Model': model_name.replace('_', ' ').title(),\n",
    "        'ROC-AUC': f\"{results['roc_auc']:.4f}\",\n",
    "        'PR-AUC': f\"{results['pr_auc']:.4f}\"\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_performance)\n",
    "print(test_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "model_trainer.plot_model_comparison(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model analysis\n",
    "best_model_name = model_trainer.best_model_name\n",
    "best_model_results = test_results[best_model_name]\n",
    "\n",
    "print(f\"BEST MODEL: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ROC-AUC: {best_model_results['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC: {best_model_results['pr_auc']:.4f}\")\n",
    "\n",
    "# Classification report for best model\n",
    "y_pred_best = best_model_results['y_pred']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Not Readmitted', 'Readmitted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Readmitted', 'Readmitted'],\n",
    "            yticklabels=['Not Readmitted', 'Readmitted'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curves for all models\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC curves\n",
    "for model_name, results in test_results.items():\n",
    "    y_pred_proba = results['y_pred_proba']\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    ax1.plot(fpr, tpr, label=f'{model_name.replace(\"_\", \" \").title()} (AUC = {results[\"roc_auc\"]:.3f})', linewidth=2)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curves - All Models', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# PR curves\n",
    "for model_name, results in test_results.items():\n",
    "    y_pred_proba = results['y_pred_proba']\n",
    "    \n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    ax2.plot(recall, precision, label=f'{model_name.replace(\"_\", \" \").title()} (AP = {results[\"pr_auc\"]:.3f})', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curves - All Models', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance for tree-based models\n",
    "print(\"Top Features by Importance:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in ['random_forest', 'xgboost', 'lightgbm']:\n",
    "    if model_name in model_trainer.feature_importance:\n",
    "        importance_df = model_trainer.feature_importance[model_name]\n",
    "        \n",
    "        print(f\"\\n{model_name.replace('_', ' ').title()} - Top 10 Features:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        top_features = importance_df.head(10)\n",
    "        for idx, row in top_features.iterrows():\n",
    "            print(f\"{row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance for best tree-based model\n",
    "if model_trainer.best_model_name in model_trainer.feature_importance:\n",
    "    importance_df = model_trainer.feature_importance[model_trainer.best_model_name]\n",
    "    top_20 = importance_df.head(20)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(top_20['feature'], top_20['importance'], color='skyblue')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Feature Importance - {model_trainer.best_model_name.replace(\"_\", \" \").title()}', \n",
    "              fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
       " Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, results in test_results.items():\n",
    "    y_pred_proba = results['y_pred_proba']\n",
    "    \n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "    \n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "             label=f'{model_name.replace(\"_\", \" \").title()}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curves - All Models', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Saving and Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "print(\"Saving trained models...\")\n",
    "model_trainer.save_models('../models/')\n",
    "print(\"✓ Models saved successfully!\")\n",
    "\n",
    "# Save feature names\n",
    "import joblib\n",
    "joblib.dump(feature_names, '../models/feature_names.pkl')\n",
    "joblib.dump(feature_engineer.preprocessor, '../models/preprocessor.pkl')\n",
    "print(\"✓ Feature names and preprocessor saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBEST MODEL: {model_trainer.best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"ROC-AUC Score: {test_results[model_trainer.best_model_name]['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC Score: {test_results[model_trainer.best_model_name]['pr_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nDATASET INFORMATION:\")\n",
    "print(f\"• Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"• Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"• Total features: {len(feature_names)}\")\n",
    "print(f\"• Feature categories: Medical, SDOH, Demographic\")\n",
    "\n",
    "print(f\"\\nMODEL COMPARISON:\")\n",
    "for model_name, results in test_results.items():\n",
    "    print(f\"• {model_name.replace('_', ' ').title()}: ROC-AUC = {results['roc_auc']:.4f}, PR-AUC = {results['pr_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nKEY SUCCESS METRICS:\")\n",
    "print(f\"✓ Integrated medical + SDOH data successfully\")\n",
    "print(f\"✓ Handled class imbalance effectively\")\n",
    "print(f\"✓ Achieved strong predictive performance\")\n",
    "print(f\"✓ All models trained and evaluated\")\n",
    "print(f\"✓ Models saved for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Model Interpretation**: Use SHAP for explainability (see interpretability notebook)\n",
    "2. **Fairness Analysis**: Evaluate model performance across demographic groups\n",
    "3. **Dashboard Deployment**: Create interactive dashboard for stakeholders\n",
    "4. **Model Monitoring**: Set up monitoring for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability: SHAP Analysis\n",
    "## MSDS692 - Data Science Practicum\n",
    "### Sai Teja Lakkapally\n",
    "\n",
    "This notebook focuses on model interpretability using SHAP values to understand feature impacts and ensure model transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from etl import DataETL\n",
    "from features import FeatureEngineer\n",
    "from model import ReadmissionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"Loading pre-trained models...\")\n",
    "\n",
    "model_trainer = ReadmissionModel()\n",
    "\n",
    "if os.path.exists('../models/'):\n",
    "    model_trainer.load_models('../models/')\n",
    "    feature_names = joblib.load('../models/feature_names.pkl')\n",
    "    preprocessor = joblib.load('../models/preprocessor.pkl')\n",
    "    print(\"âœ“ Models and features loaded successfully!\")\n",
    "else:\n",
    "    print(\"Training new models...\")\n",
    "    # Train models if not saved\n",
    "    etl = DataETL()\n",
    "    data = etl.run_pipeline()\n",
    "    \n",
    "    feature_engineer = FeatureEngineer()\n",
    "    X, y, feature_names = feature_engineer.prepare_features(data)\n",
    "    X_balanced, y_balanced = feature_engineer.handle_imbalance(X, y, method='smote')\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    "    )\n",
    "    \n",
    "    model_trainer.train_baseline_models(X_train, y_train)\n",
    "    model_trainer.train_advanced_models(X_train, y_train)\n",
    "    test_results = model_trainer.evaluate_models(X_test, y_test)\n",
    "    \n",
    "    # Save models\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    model_trainer.save_models('../models/')\n",
    "    joblib.dump(feature_names, '../models/feature_names.pkl')\n",
    "    joblib.dump(feature_engineer.preprocessor, '../models/preprocessor.pkl')\n",
    "\n",
    "print(f\"Best model: {model_trainer.best_model_name}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SHAP Analysis for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for SHAP analysis\n",
    "print(\"Preparing data for SHAP analysis...\")\n",
    "\n",
    "# Use a sample for faster computation (SHAP can be computationally intensive)\n",
    "X_sample = X_test.iloc[:1000] if 'X_test' in locals() else None\n",
    "\n",
    "if X_sample is None:\n",
    "    # Generate sample data if not available\n",
    "    etl = DataETL()\n",
    "    data = etl.run_pipeline()\n",
    "    feature_engineer = FeatureEngineer()\n",
    "    X, y, feature_names = feature_engineer.prepare_features(data)\n",
    "    X_sample = X.iloc[:1000]\n",
    "\n",
    "print(f\"SHAP analysis sample shape: {X_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SHAP analysis for best model\n",
    "best_model = model_trainer.best_model\n",
    "best_model_name = model_trainer.best_model_name\n",
    "\n",
    "print(f\"Performing SHAP analysis for {best_model_name}...\")\n",
    "\n",
    "explainer, shap_values, shap_importance = model_trainer.shap_analysis(\n",
    "    X_sample, feature_names, best_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comprehensive SHAP Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SHAP Summary Plot (Bee Swarm)\n",
    "print(\"1. SHAP Summary Plot (Bee Swarm)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)\n",
    "plt.title(f'SHAP Summary Plot - {best_model_name.replace(\"_\", \" \").title()}', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SHAP Bar Plot (Feature Importance)\n",
    "print(\"2. SHAP Feature Importance\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "plt.title(f'SHAP Feature Importance - {best_model_name.replace(\"_\", \" \").title()}', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Feature Importance Comparison\n",
    "print(\"3. Feature Importance Comparison: SHAP vs Model\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get model feature importance if available\n",
    "if best_model_name in model_trainer.feature_importance:\n",
    "    model_importance = model_trainer.feature_importance[best_model_name]\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'shap_importance': np.abs(shap_values).mean(axis=0),\n",
    "        'model_importance': model_importance.set_index('feature').reindex(feature_names)['importance'].values\n",
    "    })\n",
    "    \n",
    "    # Normalize importances\n",
    "    comparison_df['shap_importance_norm'] = comparison_df['shap_importance'] / comparison_df['shap_importance'].sum()\n",
    "    comparison_df['model_importance_norm'] = comparison_df['model_importance'] / comparison_df['model_importance'].sum()\n",
    "    \n",
    "    # Get top 15 features\n",
    "    top_features = comparison_df.nlargest(15, 'shap_importance_norm')\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    x = np.arange(len(top_features))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.barh(x - width/2, top_features['shap_importance_norm'], width, \n",
    "                    label='SHAP Importance', color='skyblue', alpha=0.8)\n",
    "    bars2 = ax.barh(x + width/2, top_features['model_importance_norm'], width, \n",
    "                    label='Model Importance', color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    ax.set_yticks(x)\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Normalized Importance')\n",
    "    ax.set_title('Feature Importance: SHAP vs Model', fontsize=16, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display correlation between importances\n",
    "    correlation = comparison_df['shap_importance_norm'].corr(comparison_df['model_importance_norm'])\n",
    "    print(f\"Correlation between SHAP and Model importance: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Impact Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features\n",
    "def categorize_feature(feature_name):\n",
    "    \"\"\"Categorize features into medical, SDOH, or demographic\"\"\"\n",
    "    medical_keywords = ['length_of_stay', 'num_', 'number_', 'prior_admissions', \n",
    "                       'comorbidity', 'utilization', 'procedure', 'medications',\n",
    "                       'diagnoses', 'lab_procedures']\n",
    "    \n",
    "    sdoh_keywords = ['svi', 'rpl', 'theme', 'vulnerability', 'economic', \n",
    "                    'hardship', 'access_barrier', 'isolation', 'pov', 'unemp',\n",
    "                    'nohsdp', 'noveh']\n",
    "    \n",
    "    demographic_keywords = ['age', 'gender', 'race']\n",
    "    \n",
    "    feature_lower = feature_name.lower()\n",
    "    \n",
    "    for keyword in medical_keywords:\n",
    "        if keyword in feature_lower:\n",
    "            return 'Medical'\n",
    "    \n",
    "    for keyword in sdoh_keywords:\n",
    "        if keyword in feature_lower:\n",
    "            return 'SDOH'\n",
    "    \n",
    "    for keyword in demographic_keywords:\n",
    "        if keyword in feature_lower:\n",
    "            return 'Demographic'\n",
    "    \n",
    "    return 'Other'\n",
    "\n",
    "# Add categories to importance data\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_importance': np.abs(shap_values).mean(axis=0)\n",
    "})\n",
    "\n",
    "shap_importance_df['category'] = shap_importance_df['feature'].apply(categorize_feature)\n",
    "shap_importance_df = shap_importance_df.sort_values('shap_importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance by Category:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "category_summary = shap_importance_df.groupby('category').agg({\n",
    "    'shap_importance': ['sum', 'mean', 'count']\n",
    "}).round(4)\n",
    "\n",
    "category_summary.columns = ['Total Importance', 'Average Importance', 'Feature Count']\n",
    "print(category_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize importance by category\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "category_importance = shap_importance_df.groupby('category')['shap_importance'].sum().sort_values(ascending=True)\n",
    "\n",
    "colors = {'Medical': '#3498db', 'SDOH': '#e74c3c', 'Demographic': '#2ecc71', 'Other': '#f39c12'}\n",
    "bar_colors = [colors.get(cat, '#95a5a6') for cat in category_importance.index]\n",
    "\n",
    "bars = plt.barh(category_importance.index, category_importance.values, color=bar_colors, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Total SHAP Importance')\n",
    "plt.title('Total Feature Importance by Category', fontsize=16, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.4f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Individual Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific predictions\n",
    "print(\"5. Individual Prediction Explanations\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get a few example predictions\n",
    "sample_indices = [0, 1, 2]  # First three samples\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    actual_label = y_test.iloc[idx] if 'y_test' in locals() else \"Unknown\"\n",
    "    prediction = best_model.predict_proba(X_sample.iloc[[idx]])[0, 1]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Predicted probability of readmission: {prediction:.4f}\")\n",
    "    print(f\"  Actual readmission status: {actual_label}\")\n",
    "    \n",
    "    # Get SHAP values for this prediction\n",
    "    shap_value_single = shap_values[idx]\n",
    "    \n",
    "    # Get top features influencing this prediction\n",
    "    feature_effects = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'shap_value': shap_value_single,\n",
    "        'abs_effect': np.abs(shap_value_single)\n",
    "    }).sort_values('abs_effect', ascending=False)\n",
    "    \n",
    "    print(\"  Top features influencing prediction:\")\n",
    "    for _, row in feature_effects.head(5).iterrows():\n",
    "        direction = \"increases\" if row['shap_value'] > 0 else \"decreases\"\n",
    "        print(f\"    â€¢ {row['feature']}: {direction} risk (impact: {row['shap_value']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot for a single prediction\n",
    "print(\"\\nForce Plot for Sample Prediction:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Use the first sample for demonstration\n",
    "sample_idx = 0\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1] if hasattr(explainer.expected_value, '__len__') else explainer.expected_value,\n",
    "    shap_values[sample_idx],\n",
    "    X_sample.iloc[sample_idx],\n",
    "    feature_names=feature_names,\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "plt.title(f'SHAP Force Plot - Sample {sample_idx}', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dependence Plots for Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots for top features\n",
    "print(\"6. Feature Dependence Plots\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "top_features = shap_importance_df.head(6)['feature'].tolist()\n",
    "\n",
    "for i, feature in enumerate(top_features[:3]):  # Plot first 3 for clarity\n",
    "    print(f\"\\nDependence plot for: {feature}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(\n",
    "        feature, \n",
    "        shap_values, \n",
    "        X_sample, \n",
    "        feature_names=feature_names,\n",
    "        show=False\n",
    "    )\n",
    "    \n",
    "    plt.title(f'SHAP Dependence Plot: {feature}', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SDOH Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on SDOH features\n",
    "print(\"7. Social Determinants of Health Impact Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sdoh_features = shap_importance_df[shap_importance_df['category'] == 'SDOH']\n",
    "\n",
    "print(\"Top SDOH Features by Impact:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for _, row in sdoh_features.head(10).iterrows():\n",
    "    print(f\"{row['feature']}: {row['shap_importance']:.4f}\")\n",
    "\n",
    "# Visualize SDOH feature impacts\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "top_sdoh = sdoh_features.head(10)\n",
    "\n",
    "# Get the actual SHAP values for these features to see direction\n",
    "sdoh_impacts = []\n",
    "for feature in top_sdoh['feature']:\n",
    "    feature_idx = feature_names.index(feature)\n",
    "    mean_impact = shap_values[:, feature_idx].mean()\n",
    "    sdoh_impacts.append(mean_impact)\n",
    "\n",
    "colors = ['red' if impact > 0 else 'blue' for impact in sdoh_impacts]\n",
    "\n",
    "bars = plt.barh(top_sdoh['feature'], top_sdoh['shap_importance'], color=colors, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Mean |SHAP Value| (Impact Magnitude)')\n",
    "plt.title('Top SDOH Features: Impact on Readmission Prediction', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, impact in zip(bars, sdoh_impacts):\n",
    "    width = bar.get_width()\n",
    "    direction = \"â†‘ increases\" if impact > 0 else \"â†“ decreases\"\n",
    "    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.4f} ({direction})', ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Waterfall Plot for High-Risk Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a high-risk prediction for detailed explanation\n",
    "print(\"8. Waterfall Plot for High-Risk Patient\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'y_test' in locals():\n",
    "    # Find a patient with high predicted risk\n",
    "    high_risk_idx = best_model.predict_proba(X_sample)[:, 1].argmax()\n",
    "    high_risk_prob = best_model.predict_proba(X_sample.iloc[[high_risk_idx]])[0, 1]\n",
    "    actual_status = y_test.iloc[high_risk_idx]\n",
    "    \n",
    "    print(f\"High-risk patient analysis:\")\n",
    "    print(f\"  Predicted readmission probability: {high_risk_prob:.4f}\")\n",
    "    print(f\"  Actual readmission status: {actual_status}\")\n",
    "    \n",
    "    # Create waterfall plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(\n",
    "            values=shap_values[high_risk_idx],\n",
    "            base_values=explainer.expected_value[1] if hasattr(explainer.expected_value, '__len__') else explainer.expected_value,\n",
    "            data=X_sample.iloc[high_risk_idx],\n",
    "            feature_names=feature_names\n",
    "        ),\n",
    "        max_display=15,\n",
    "        show=False\n",
    "    )\n",
    "    \n",
    "    plt.title(f'SHAP Waterfall Plot: High-Risk Patient (Probability: {high_risk_prob:.3f})', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Test labels not available for high-risk patient analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final interpretability summary\n",
    "print(\"MODEL INTERPRETABILITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. OVERALL INSIGHTS:\")\n",
    "print(f\"   â€¢ Best Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"   â€¢ Total Features Analyzed: {len(feature_names)}\")\n",
    "print(f\"   â€¢ Samples for SHAP: {X_sample.shape[0]}\")\n",
    "\n",
    "print(f\"\\n2. FEATURE CATEGORY IMPACT:\")\n",
    "category_impact = shap_importance_df.groupby('category')['shap_importance'].sum()\n",
    "total_impact = category_impact.sum()\n",
    "\n",
    "for category, impact in category_impact.items():\n",
    "    percentage = (impact / total_impact) * 100\n",
    "    print(f\"   â€¢ {category}: {percentage:.1f}% of total impact\")\n",
    "\n",
    "print(f\"\\n3. TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "top_5 = shap_importance_df.head(5)\n",
    "for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['feature']} ({row['category']}): {row['shap_importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. SDOH IMPACT CONFIRMATION:\")\n",
    "sdoh_total_impact = category_impact.get('SDOH', 0)\n",
    "sdoh_percentage = (sdoh_total_impact / total_impact) * 100\n",
    "print(f\"   â€¢ SDOH features contribute {sdoh_percentage:.1f}% to model predictions\")\n",
    "print(f\"   â€¢ This confirms the importance of social factors in readmission risk\")\n",
    "\n",
    "print(f\"\\n5. MODEL TRANSPARENCY:\")\n",
    "print(f\"   âœ“ Feature impacts quantified and visualized\")\n",
    "print(f\"   âœ“ Individual predictions explainable\")\n",
    "print(f\"   âœ“ SDOH impact clearly demonstrated\")\n",
    "print(f\"   âœ“ Model decisions are interpretable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETABILITY ANALYSIS COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Business Implications\n",
    "\n",
    "1. **Clinical Decision Support**: The model identifies both clinical and social risk factors, enabling comprehensive patient assessment.\n",
    "\n",
    "2. **Resource Allocation**: Healthcare systems can prioritize interventions based on both medical needs and social vulnerabilities.\n",
    "\n",
    "3. **Policy Insights**: The significant impact of SDOH features supports policies addressing social determinants.\n",
    "\n",
    "4. **Health Equity**: Transparent model explanations help ensure fair and equitable care decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}